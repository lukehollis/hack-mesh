using System;
using UnityEngine;
using UnityEngine.SceneManagement;
using Unity.MLAgents;
using UnityEngine.Serialization;

/// <summary>
/// An example of how to use ML-Agents without inheriting from the Agent class.
/// Observations are generated by the attached SensorComponent, and the actions
/// are retrieved from the Agent.
/// </summary>
public class SwallowController : MonoBehaviour
{
    [SerializeField]
    private SwallowFlockManager flockManager;
    
    public float timeBetweenDecisionsAtInference;
    float m_TimeSinceDecision;
    [FormerlySerializedAs("m_Position")]
    [HideInInspector]
    public Vector3 position;
    public Vector3 startPosition;
    public Vector3 smallGoalPosition;
    public Vector3 largeGoalPosition;
    public GameObject largeGoal;
    public GameObject smallGoal;
    public const float minX = -10f;
    public const float maxX = 10f;
    public const float minY = 0f;
    public const float maxY = 15f;
    public const float minZ = -10f;
    public const float maxZ = 10f;
    public const int k_Extents = 20;
    public float delayBeforeFlying = 1f;  // Time in seconds before bird starts flying
    private float flyingDelayTimer = 0f;
    private bool canFly = false;

    Agent m_Agent;

    private Vector3 currentPosition;
    private Vector3 targetPosition;
    public float lerpSpeed = 1f;
    private bool isLerping = false;
    // private float respawnTimer = 0f;
    private const float RESPAWN_TIME = 30f;

    public int discretizedPosition
    {
        get
        {
            int x = Mathf.RoundToInt(Mathf.Clamp((position.x - minX) / (maxX - minX) * k_Extents, 0, k_Extents - 1));
            int y = Mathf.RoundToInt(Mathf.Clamp((position.y - minY) / (maxY - minY) * k_Extents, 0, k_Extents - 1));
            int z = Mathf.RoundToInt(Mathf.Clamp((position.z - minZ) / (maxZ - minZ) * k_Extents, 0, k_Extents - 1));
            
            return x + y * k_Extents + z * k_Extents * k_Extents;
        }
    }

    public void Awake()
    {
        // Since this example does not inherit from the Agent class, explicit registration
        // of the RpcCommunicator is required. The RPCCommunicator should only be compiled
        // for Standalone platforms (i.e. Windows, Linux, or Mac)
#if UNITY_EDITOR || UNITY_STANDALONE
        if (!CommunicatorFactory.CommunicatorRegistered)
        {
            Debug.Log("Registered Communicator.");
            CommunicatorFactory.Register<ICommunicator>(RpcCommunicator.Create);
        }
#endif
    }

    public void OnEnable()
    {
        m_Agent = GetComponent<Agent>();
        currentPosition = transform.position;
        targetPosition = currentPosition;
        position = currentPosition;
        startPosition = position;
        smallGoalPosition = smallGoal.transform.position;
        largeGoalPosition = largeGoal.transform.position;
        
        // Reset flying state
        flyingDelayTimer = 0f;
        canFly = false;
    }

    /// <summary>
    /// Controls the movement of the GameObject based on the actions received.
    /// </summary>
    /// <param name="direction"></param>
    public void MoveDirection(Vector3 direction)
    {
        if (!canFly)
            return;
        
        // Get flocking influence
        Vector3 flockInfluence = flockManager.CalculateFlockingBehavior(this);
        
        // Combine ML-Agents direction with flocking behavior
        Vector3 combinedDirection = (direction + flockInfluence).normalized;
        
        // Calculate new target position
        targetPosition = position + combinedDirection;
        targetPosition.x = Mathf.Clamp(targetPosition.x, minX, maxX);
        targetPosition.y = Mathf.Clamp(targetPosition.y, minY, maxY);
        targetPosition.z = Mathf.Clamp(targetPosition.z, minZ, maxZ);
        
        isLerping = true;
        
        // Add group behavior reward
        float groupReward = Vector3.Dot(direction.normalized, flockInfluence.normalized) * 0.005f;
        m_Agent.AddReward(-0.01f + groupReward);
    }

    public void ResetAgent()
    {
        position = startPosition;
        transform.position = startPosition;
        // respawnTimer = 0f;
        
        // Reset flying state
        // flyingDelayTimer = 0f;
        canFly = false;

        // This is a very inefficient way to reset the scene. Used here for testing. Is likely
        // causing undisposed tensor issue with Sentis.
        // SceneManager.LoadScene(SceneManager.GetActiveScene().name);
        // m_Agent = null; // LoadScene only takes effect at the next Update.
        // We set the Agent to null to avoid using the Agent before the reload
    }

    public void FixedUpdate()
    {
        WaitTimeInference();
    }

    void WaitTimeInference()
    {
        if (m_Agent == null)
        {
            return;
        }
        if (Academy.Instance.IsCommunicatorOn)
        {
            m_Agent?.RequestDecision();
        }
        else
        {
            if (m_TimeSinceDecision >= timeBetweenDecisionsAtInference)
            {
                m_TimeSinceDecision = 0f;
                m_Agent?.RequestDecision();
            }
            else
            {
                m_TimeSinceDecision += Time.fixedDeltaTime;
            }
        }
    }

    void Update()
    {
        if (!canFly)
        {
            flyingDelayTimer += Time.deltaTime;
            if (flyingDelayTimer >= delayBeforeFlying)
            {
                canFly = true;
            }
        }

        if (isLerping)
        {
            // Smoothly lerp to target position
            position = Vector3.Lerp(position, targetPosition, Time.deltaTime * lerpSpeed);
            transform.position = position;

            // Check if we're close enough to target to stop lerping
            if (Vector3.Distance(position, targetPosition) < 0.01f)
            {
                position = targetPosition;
                transform.position = position;
                isLerping = false;
            }
        }

        // Add respawn timer
        // respawnTimer += Time.deltaTime;
        // if (respawnTimer >= RESPAWN_TIME)
        // {
        //     ResetAgent();
        //     respawnTimer = 0f;
        // }

        // Check for goal conditions
        if (Vector3.Distance(position, smallGoalPosition) < 1f)
        {
            m_Agent.AddReward(0.1f);
            m_Agent.EndEpisode();
            ResetAgent();
        }

        if (Vector3.Distance(position, largeGoalPosition) < 1f)
        {
            m_Agent.AddReward(1f);
            m_Agent.EndEpisode();
            ResetAgent();
        }
    }
}
